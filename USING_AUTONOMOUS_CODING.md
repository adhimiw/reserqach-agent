# Using the Autonomous Coding System

## What You Asked For vs What You Got

**You asked:**
> "add this into my agent do precoded logic agent has to code it slef and save it and run it give the dam natve terminal compelte access to use it, make the agent more powerfull with this"

**You got:**
âœ… Agent writes its own code (NO pre-coded logic)
âœ… Agent saves code as `.py` files
âœ… Agent executes its own code
âœ… Full terminal access with subprocess control
âœ… Agent creates and controls its own Python environment
âœ… Much more powerful - unlimited flexibility!

## Quick Start (5 Minutes)

### Step 1: Use the dataset you mentioned

You have `1vddd.csv` from your earlier run. Let's use that:

```bash
cd /home/engine/project
source venv/bin/activate

# Run autonomous analysis - agent will write and execute its own code
python main_autonomous.py 1vddd.csv
```

### Step 2: With Target Column (Model Building)

```bash
python main_autonomous.py 1vddd.csv --target [your_target_column]
```

### Step 3: With LLM Council + Autonomous Coding (BEST)

```bash
python main_autonomous_with_council.py 1vddd.csv --target [your_target_column]
```

## What Actually Happens

### Old Way (Pre-coded Logic)
```
User runs analysis
  â†“
System uses pre-written functions
  â†“
Results generated
  â†“
âŒ No code visibility
âŒ Can't modify analysis
âŒ Limited to what developers coded
```

### New Way (Autonomous Coding)
```
User runs analysis
  â†“
Agent creates isolated Python environment
  â†“
Agent uses LLM to generate Python code
  â†“
Agent saves code as timestamped .py file
  â†“
Agent executes its generated code
  â†“
âœ“ All code saved and visible
âœ“ Can edit and re-run code
âœ“ Unlimited possibilities
```

## Example: What Gets Generated

When you run `python main_autonomous.py 1vddd.csv`, the agent creates:

### 1. Exploratory Analysis Code
```python
# File: output/analyses/1vddd/20260126_115941/generated_code/exploratory_analysis_20260126_115941.py

"""
Generated by Autonomous Code Execution Agent
Filename: exploratory_analysis_20260126_115941.py
Generated: 2026-01-26T11:59:41.123456
Description: Exploratory data analysis with visualizations
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os

# Load dataset
df = pd.read_csv('data/original.csv')

print("="*60)
print("EXPLORATORY DATA ANALYSIS")
print("="*60)

# Dataset info
print(f"\nDataset Shape: {df.shape}")
print(f"\nColumns: {list(df.columns)}")

# Summary statistics
print(f"\nSummary Statistics:\n{df.describe()}")

# Missing values
missing = df.isnull().sum()
print(f"\nMissing Values:\n{missing[missing > 0]}")

# Visualizations
output_dir = 'visualizations'
os.makedirs(output_dir, exist_ok=True)

# Distribution plots
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols[:5]:
    plt.figure(figsize=(8, 5))
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f'Distribution of {col}')
    plt.savefig(f'{output_dir}/distribution_{col}.png')
    plt.close()

# Correlation heatmap
if len(numeric_cols) > 1:
    plt.figure(figsize=(12, 8))
    sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation Heatmap')
    plt.savefig(f'{output_dir}/correlation_heatmap.png')
    plt.close()

# Save summary
summary = {
    'shape': df.shape,
    'columns': list(df.columns),
    'numeric_columns': list(numeric_cols),
    'missing_values': missing.to_dict()
}

with open('data/eda_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)

print("\nâœ“ Exploratory analysis complete")
print(f"âœ“ Visualizations saved to {output_dir}")
print(f"âœ“ Summary saved to data/eda_summary.json")
```

### 2. Feature Engineering Code
```python
# File: output/analyses/1vddd/20260126_115942/generated_code/feature_engineering_20260126_115942.py

"""
Generated by Autonomous Code Execution Agent
Filename: feature_engineering_20260126_115942.py
Description: Advanced feature engineering pipeline
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
import json

df = pd.read_csv('data/original.csv')

print("="*60)
print("FEATURE ENGINEERING")
print("="*60)

# Identify column types
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"Numeric columns: {numeric_cols}")
print(f"Categorical columns: {categorical_cols}")

# Handle missing values
print("\nHandling missing values...")
for col in numeric_cols:
    df[col].fillna(df[col].median(), inplace=True)

for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Encode categorical variables
print("\nEncoding categorical variables...")
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Scale numeric features
print("\nScaling numeric features...")
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Create interaction features
print("\nCreating interaction features...")
if len(numeric_cols) >= 2:
    df['feature_interaction'] = df[numeric_cols[0]] * df[numeric_cols[1]]

# Save engineered dataset
df.to_csv('data/engineered_data.csv', index=False)

print("\nâœ“ Feature engineering complete")
print(f"âœ“ Engineered dataset saved to data/engineered_data.csv")
```

### 3. Model Building Code (if target column provided)
```python
# File: output/analyses/1vddd/20260126_115943/generated_code/model_building_20260126_115943.py

"""
Generated by Autonomous Code Execution Agent
Filename: model_building_20260126_115943.py
Description: Model training and evaluation
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib

df = pd.read_csv('data/engineered_data.csv')
target_column = 'churn'  # Example target

print("="*60)
print("MODEL BUILDING")
print("="*60)

# Prepare data
X = df.drop(columns=[target_column])
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100)
}

results = {}
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = roc_auc_score(y_test, y_pred)
    results[name] = {
        'model': str(model),
        'roc_auc': float(score),
        'classification_report': classification_report(y_test, y_pred, output_dict=True)
    }
    print(f"âœ“ {name} - ROC AUC: {score:.4f}")

# Find best model
best_model_name = max(results, key=lambda x: results[x]['roc_auc'])
best_model = models[best_model_name]

# Save best model
joblib.dump(best_model, 'models/best_model.pkl')

print(f"\nâœ“ Best model: {best_model_name}")
print(f"âœ“ Model saved to models/best_model.pkl")

# Feature importance
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])
    plt.title('Top 10 Feature Importances')
    plt.savefig('visualizations/feature_importance.png')
    print("âœ“ Feature importance plot saved")
```

## Viewing Generated Code

After running analysis, you can see ALL the code the agent generated:

```bash
# List all generated code files
ls -lh output/analyses/1vddd/*/generated_code/

# View a specific file
cat output/analyses/1vddd/*/generated_code/exploratory_analysis_*.py

# Edit the code if you want
nano output/analyses/1vddd/*/generated_code/exploratory_analysis_*.py

# Re-run the modified code
python output/analyses/1vddd/*/generated_code/exploratory_analysis_*.py
```

## Running Generated Code Independently

Each generated file is completely self-contained and can be run anywhere:

```bash
# Copy generated code to another directory
cp output/analyses/1vddd/*/generated_code/exploratory_analysis_*.py ~/my_analysis/

# Put your data file there
cp output/analyses/1vddd/*/data/original.csv ~/my_analysis/data.csv

# Run the code
cd ~/my_analysis
python exploratory_analysis_*.py
```

## Environment Control

The agent creates its own isolated environment:

```bash
# View the environment
ls -la output/analyses/1vddd/*/envs/analysis_env/

# Activate the environment
source output/analyses/1vddd/*/envs/analysis_env/bin/activate

# See installed packages
pip list

# Add more packages
pip install xgboost lightgbm
```

## Terminal Access

The agent has full terminal access:

```python
# In the generated code or via CodeExecutionAgent
executor.run_terminal_command("ls -la")
executor.run_terminal_command("pip list | grep pandas")
executor.run_terminal_command("python -c 'import numpy; print(numpy.pi)'")
executor.run_terminal_command("cat data.csv | head -5")
```

## Jupyter Notebook Generation

The agent can compile all generated code into a Jupyter notebook:

```bash
# Run with notebook generation (default)
python main_autonomous.py 1vddd.csv

# Open the notebook
jupyter notebook output/analyses/1vddd/*/notebooks/analysis_*.ipynb
```

The notebook contains:
- All generated code cells
- All outputs and visualizations
- Complete analysis pipeline

## Comparison: Your Dataset (1vddd.csv)

### Old Way (main_with_council.py)
```bash
python main_with_council.py 1vddd.csv
```
- Uses pre-coded functions in analysis_engine/
- No code files saved
- Can't see what the system actually did
- Hard to reproduce or modify

### New Way (main_autonomous_with_council.py)
```bash
python main_autonomous_with_council.py 1vddd.csv --target [your_target_column]
```
- Agent generates custom Python code for YOUR dataset
- All code saved as timestamped `.py` files
- Complete transparency - you can see every line of code
- Easy to modify and re-run
- Plus LLM Council consensus for better decisions

## Output Directory for Your Dataset

```
output/analyses/1vddd/20260126_115941/
â”‚
â”œâ”€â”€ generated_code/                    â† Agent wrote all this code!
â”‚   â”œâ”€â”€ exploratory_analysis_20260126_115941.py    # Agent wrote this
â”‚   â”œâ”€â”€ feature_engineering_20260126_115942.py     # Agent wrote this
â”‚   â”œâ”€â”€ model_building_20260126_115943.py          # Agent wrote this
â”‚   â””â”€â”€ code_manifest.json                          # List of all files
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis_20260126_115941.ipynb           # Compiled notebook
â”‚
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ distributions/
â”‚   â”œâ”€â”€ correlations/
â”‚   â””â”€â”€ feature_importance.png
â”‚
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ analysis_env/                   â† Agent created this environment
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ original.csv
â”‚   â””â”€â”€ engineered_data.csv
â”‚
â””â”€â”€ autonomous_analysis_results.json    â† Complete results
```

## Running Your Specific Dataset

```bash
cd /home/engine/project
source venv/bin/activate

# Basic autonomous analysis (agent writes and executes code)
python main_autonomous.py 1vddd.csv

# With target column for predictive modeling
python main_autonomous.py 1vddd.csv --target [your_target_column]

# With LLM Council guidance AND autonomous coding (most powerful)
python main_autonomous_with_council.py 1vddd.csv --target [your_target_column]
```

## What This Means for You

### Before (Old System)
âŒ Agent uses pre-coded logic
âŒ Can't see the code
âŒ Can't modify analysis
âŒ Limited to what developers imagined
âŒ No code persistence
âŒ Hard to reproduce results

### After (New Autonomous System)
âœ… Agent generates custom code for YOUR dataset
âœ… All code visible in `.py` files
âœ… Can edit and improve generated code
âœ… Unlimited - can do anything Python can do
âœ… Complete code persistence and history
âœ… Fully reproducible - just re-run the `.py` files
âœ… Full terminal access
âœ… Agent controls its own environment
âœ… Much more powerful!

## Next Steps

1. **Run the autonomous analysis** on your dataset
2. **View the generated code** in `generated_code/` directory
3. **Open the Jupyter notebook** to see the full analysis
4. **Edit the code** if you want to customize anything
5. **Re-run the code** to see your changes

## Documentation

- **AUTONOMOUS_CODING_README.md** - Complete technical documentation
- **QUICKSTART_AUTONOMOUS.md** - 5-minute quick start guide
- **test_autonomous_coding.py** - Test suite to verify everything works

## Summary

You now have a system where:
- ğŸ¤– The agent writes its own Python code
- ğŸ’¾ The code is saved as `.py` files (fully visible)
- âš™ï¸ The agent creates and controls its own environment
- ğŸ’» The agent has full terminal access
- ğŸš€ The agent is much more powerful (unlimited possibilities)

This is exactly what you asked for - no pre-coded logic, full agent autonomy with code generation and execution capabilities!
