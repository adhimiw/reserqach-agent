"""
LLM Council Integration for Data Science System
Integrates LLM Council (multi-agent consensus) into analysis workflow
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
import logging
import sys
import os

# Add project to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import pandas (lazy, only if needed for actual data processing)
try:
    import pandas as pd
    import numpy as np
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    print("Note: pandas not available, running structural tests only")

# Import LLM API management modules (with graceful fallback)
try:
    from llm_api_integration import LLMAPIManager, TokenLogger, EndpointHealthChecker
    HAS_API_MANAGER = True
except ImportError:
    HAS_API_MANAGER = False
    LLMAPIManager = None
    TokenLogger = None
    EndpointHealthChecker = None

logger = logging.getLogger("LLMCouncilIntegration")


class LLMCouncilAdapter:
    """Adapter for LLM Council to work with Data Science System"""
    
    def __init__(self, council_backend_path: str = None):
        """
        Initialize LLM Council adapter
        
        Args:
            council_backend_path: Path to llm-council backend
        """
        self.council_backend_path = council_backend_path or "/home/engine/project/llm-council/backend"
        self.enabled = True
        
        # Initialize API manager
        self.api_manager = None
        self.token_logger = None
        self.health_checker = None
        
        if HAS_API_MANAGER:
            # Mock config for API manager
            mock_config = {
                "PERPLEXITY_API_KEY": os.getenv("PERPLEXITY_API_KEY", ""),
                "MISTRAL_API_KEY": os.getenv("MISTRAL_API_KEY", ""),
                "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY", ""),
                "OPENROUTER_API_KEY": os.getenv("OPENROUTER_API_KEY", ""),
            }
            
            try:
                self.api_manager = LLMAPIManager(config=mock_config)
                self.token_logger = TokenLogger()
                self.health_checker = EndpointHealthChecker(api_manager=self.api_manager)
                logger.info("LLM API manager initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize API manager: {e}")
        
        logger.info(f"LLM Council adapter initialized with backend: {self.council_backend_path}")
    
    async def generate_hypotheses_with_council(self, dataset_info: Dict[str, Any],
                                           max_hypotheses: int = 50) -> List[Dict[str, Any]]:
        """
        Generate hypotheses using LLM Council consensus
        
        Args:
            dataset_info: Dataset information (shape, columns, types, etc.)
            max_hypotheses: Maximum number of hypotheses to generate
        
        Returns:
            List of hypotheses generated by council consensus
        """
        if not self.enabled:
            logger.info("LLM Council disabled, using single LLM")
            return []
        
        logger.info(f"Generating hypotheses with LLM Council consensus (max: {max_hypotheses})")
        
        # Build prompt for hypothesis generation
        prompt = f"""You are a team of expert data scientists analyzing a dataset.

Dataset Information:
- Shape: {dataset_info.get('shape', 'Unknown')}
- Columns: {dataset_info.get('columns', [])}
- Data Types: {dataset_info.get('dtypes', {})}

Your task is to generate {max_hypotheses} testable hypotheses about this dataset.

For each hypothesis, provide:
1. **Type**: correlation, distribution, categorical, outlier, or trend
2. **Columns involved**: Which columns this hypothesis relates to
3. **Hypothesis statement**: Clear testable statement
4. **Test method**: Statistical test to validate this hypothesis
5. **Reasoning**: Why this hypothesis is worth testing

Generate diverse hypotheses covering different aspects of data. Focus on insights that would be actionable for a business or research context.

Format each hypothesis as a JSON object with these fields: type, columns, hypothesis, test_method, reasoning.

Return all hypotheses as a JSON array.
"""
        
        # Check if we have API manager available for LLM Council
        if not self.api_manager:
            logger.warning("API manager not available, cannot use LLM Council")
            # Fallback to mock hypotheses
            return self._generate_mock_hypotheses(dataset_info, max_hypotheses)
        
        # Use LLM Council backend
        try:
            # Try to import from llm-council backend
            sys.path.insert(0, self.council_backend_path)
            
            # Import the local council module
            import council
            
            # Run LLM Council for hypothesis generation
            stage1_results, stage2_results, stage3_result, metadata = await council.run_full_council(prompt)
            
            # Parse final synthesis for hypotheses
            hypotheses = []
            
            if stage3_result and 'response' in stage3_result:
                final_response = stage3_result['response']
                
                # Try to extract JSON from final response
                import json
                import re
                
                # Look for JSON blocks
                json_matches = re.findall(r'\{[^{}]*\}', final_response)
                
                for json_match in json_matches:
                    try:
                        # Parse as JSON array
                        hypotheses_data = json.loads(f"[{json_match}]")
                        
                        if isinstance(hypotheses_data, list):
                            # Filter and structure each hypothesis
                            for h_data in hypotheses_data[:max_hypotheses]:
                                if isinstance(h_data, dict):
                                    hypothesis = {
                                        "id": f"consensus_hypothesis_{len(hypotheses) + 1}",
                                        "type": h_data.get("type", self._guess_hypothesis_type(str(h_data.get("hypothesis", "")))),
                                        "columns": h_data.get("columns", []),
                                        "hypothesis": h_data.get("hypothesis", str(h_data.get("hypothesis", "N/A")))[:500],
                                        "test_method": h_data.get("test_method", "To be determined"),
                                        "reasoning": f"Generated by LLM Council consensus from {metadata.get('label_to_model', {}).get('Response A', 'unknown')}'s perspective and other LLM members"
                                    }
                                    hypotheses.append(hypothesis)
                                elif isinstance(h_data, str):
                                    hypothesis = {
                                        "id": f"consensus_hypothesis_{len(hypotheses) + 1}",
                                        "type": self._guess_hypothesis_type(h_data),
                                        "columns": [],
                                        "hypothesis": h_data[:500],
                                        "test_method": "To be determined",
                                        "reasoning": "Generated by LLM Council consensus"
                                    }
                                    hypotheses.append(hypothesis)
                                elif isinstance(h_data, list):
                                    for i, h_item in enumerate(h_data):
                                        if isinstance(h_item, dict):
                                            hypothesis = {
                                                "id": f"consensus_hypothesis_{len(hypotheses) + 1}",
                                                "type": h_item.get("type", self._guess_hypothesis_type(str(h_item.get("hypothesis", "")))),
                                                "columns": h_item.get("columns", []),
                                                "hypothesis": str(h_item.get("hypothesis", "N/A"))[:500],
                                                "test_method": h_item.get("test_method", "To be determined"),
                                                "reasoning": "Generated by LLM Council consensus"
                                            }
                                            hypotheses.append(hypothesis)
                
                    except json.JSONDecodeError:
                        logger.warning(f"Failed to parse hypotheses JSON from council response")
            
            # Log token usage if available
            if self.token_logger and metadata.get('duration_ms'):
                # Estimate tokens for the operation
                estimated_tokens = int(metadata.get('duration_ms', 0) / 10)  # Rough estimate
                self.token_logger.log_request(
                    provider="council_backend",
                    model="unknown",
                    prompt_tokens=estimated_tokens * 2,  # Prompt + response
                    completion_tokens=estimated_tokens,
                    cost=0.0,  # Council backend is external
                    duration_ms=metadata.get('duration_ms', 0),
                    success=True
                )
            
            logger.info(f"Generated {len(hypotheses)} hypotheses using LLM Council")
            return hypotheses
            
        except Exception as e:
            logger.error(f"Error in LLM Council hypothesis generation: {e}")
            # Fallback to mock hypotheses
            return self._generate_mock_hypotheses(dataset_info, max_hypotheses)
    
    def _generate_mock_hypotheses(self, dataset_info: Dict[str, Any], 
                               max_hypotheses: int) -> List[Dict[str, Any]]:
        """Generate mock hypotheses when LLM Council is unavailable"""
        logger.info("Generating mock hypotheses (LLM Council not available)")
        
        # Create realistic mock hypotheses based on dataset info
        columns = dataset_info.get('columns', [])
        dtypes = dataset_info.get('dtypes', {})
        numeric_cols = [col for col, dtype in dtypes.items() if 'int' in dtype or 'float' in dtype]
        categorical_cols = [col for col, dtype in dtypes.items() if dtype == 'object']
        
        mock_hypotheses = []
        
        # Generate correlation hypotheses
        for i, col1 in enumerate(numeric_cols[:min(10, len(numeric_cols)-1)]):
            if i < len(numeric_cols) - 1:
                col2 = numeric_cols[i + 1]
                mock_hypotheses.append({
                    "id": f"mock_hypothesis_{len(mock_hypotheses) + 1}",
                    "type": "correlation",
                    "columns": [col1, col2],
                    "hypothesis": f"Potential correlation exists between {col1} and {col2}",
                    "test_method": "Pearson correlation test",
                    "reasoning": "Based on data types and column analysis, a correlation relationship may exist between these numerical columns"
                })
        
        # Generate distribution hypotheses
        for col in numeric_cols[:5]:
            mock_hypotheses.append({
                "id": f"mock_hypothesis_{len(mock_hypotheses) + 1}",
                "type": "distribution",
                "columns": [col],
                "hypothesis": f"{col} may not follow a normal distribution",
                "test_method": "Shapiro-Wilk normality test",
                "reasoning": f"Categorical analysis of {col} would reveal distribution characteristics"
            })
        
        # Generate categorical hypotheses
        for col in categorical_cols[:5]:
            unique_count = dataset_info.get('shape', [0, 0])[1] // len(categorical_cols) if len(categorical_cols) > 0 else 10
            mock_hypotheses.append({
                "id": f"mock_hypothesis_{len(mock_hypotheses) + 1}",
                "type": "categorical",
                "columns": [col],
                "hypothesis": f"Different values in {col} may have distinct patterns",
                "test_method": "Chi-square test of independence",
                "reasoning": f"With {unique_count} unique values, chi-square tests could reveal associations"
            })
        
        return mock_hypotheses[:max_hypotheses]
    
    def _guess_hypothesis_type(self, text: str) -> str:
        """Guess hypothesis type from text content"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['correlation', 'relationship', 'associated', 'linked']):
            return "correlation"
        elif any(word in text_lower for word in ['distribution', 'normal', 'skewed', 'spread']):
            return "distribution"
        elif any(word in text_lower for word in ['category', 'group', 'segment', 'type']):
            return "categorical"
        elif any(word in text_lower for word in ['outlier', 'anomaly', 'extreme', 'unusual']):
            return "outlier"
        elif any(word in text_lower for word in ['trend', 'increase', 'decrease', 'over time', 'temporal', 'time series']):
            return "trend"
        elif any(word in text_lower for word in ['predict', 'forecast', 'model', 'regress']):
            return "model"
        else:
            return "general"
    
    async def generate_insights_with_council(self, analysis_results: Dict[str, Any],
                                         min_insights: int = 50) -> List[Dict[str, Any]]:
        """
        Generate insights using LLM Council consensus
        
        Args:
            analysis_results: Analysis results (statistical tests, models, etc.)
            min_insights: Minimum number of insights to generate
        
        Returns:
            List of insights generated by council consensus
        """
        if not self.enabled:
            logger.info("LLM Council disabled, using single LLM")
            return []
        
        logger.info(f"Generating insights with LLM Council consensus (min: {min_insights})")
        
        # Build prompt for insight generation
        statistical_summary = self._summarize_statistical_results(analysis_results)
        
        prompt = f"""You are a team of expert data analysts reviewing analysis results.

Analysis Summary:
{statistical_summary}

Your task is to generate at least {min_insights} actionable insights from these results.

For each insight, provide:
1. **Title**: Short descriptive title
2. **Type**: correlation, distribution, outlier, statistical_test, model_performance, or data_quality
3. **What**: Clear statement of the finding
4. **Why**: Explanation of the underlying reason (data-driven)
5. **How**: Practical application and how to use this insight
6. **Recommendation**: Actionable next steps

Focus on insights that are:
- Actionable for business or research decisions
- Backed by statistical evidence
- Explained in clear, non-technical language
- Have clear "why" and "how" components

Format each insight as a JSON object with these fields: title, type, what, why, how, recommendation.

Return all insights as a JSON array.
"""
        
        if not self.api_manager:
            logger.warning("API manager not available, cannot use LLM Council")
            return self._generate_mock_insights(analysis_results, min_insights)
        
        # Use LLM Council for insight generation
        try:
            sys.path.insert(0, self.council_backend_path)
            # Import the local council module
            import council
            
            # Run LLM Council for insight generation
            stage1_results, stage2_results, stage3_result, metadata = await council.run_full_council(prompt)
            
            # Parse final synthesis for insights
            insights = []
            
            if stage3_result and 'response' in stage3_result:
                final_response = stage3_result['response']
                
                # Try to extract JSON from final response
                import json
                import re
                
                # Look for JSON blocks
                json_matches = re.findall(r'\{[^{}]*\}', final_response)
                
                for json_match in json_matches:
                    try:
                        # Parse as JSON array
                        insights_data = json.loads(f"[{json_match}]")
                        
                        if isinstance(insights_data, list):
                            # Filter and structure each insight
                            for i_data in insights_data[:min_insights]:
                                if isinstance(i_data, dict):
                                    insight = {
                                        "id": f"consensus_insight_{len(insights) + 1}",
                                        "title": str(i_data.get("title", "N/A"))[:200],
                                        "type": i_data.get("type", self._guess_insight_type(str(i_data.get("what", "")))),
                                        "what": str(i_data.get("what", "N/A"))[:500],
                                        "why": str(i_data.get("why", "Generated by LLM Council consensus from peer review of analysis results"))[:500],
                                        "how": str(i_data.get("how", "Apply findings according to context and requirements"))[:500],
                                        "recommendation": str(i_data.get("recommendation", "Use this insight to inform decision-making"))[:500]
                                    }
                                    insights.append(insight)
                                elif isinstance(i_data, str):
                                    insight = {
                                        "id": f"consensus_insight_{len(insights) + 1}",
                                        "title": i_data[:200],
                                        "type": "general",
                                        "what": i_data[:500],
                                        "why": "Generated by LLM Council consensus",
                                        "how": "Apply according to context",
                                        "recommendation": "Use to inform decisions"
                                    }
                                    insights.append(insight)
                
                    except json.JSONDecodeError:
                        logger.warning("Failed to parse insights JSON from council response")
            
            logger.info(f"Generated {len(insights)} insights using LLM Council")
            return insights
            
        except Exception as e:
            logger.error(f"Error in LLM Council insight generation: {e}")
            return self._generate_mock_insights(analysis_results, min_insights)
    
    def _generate_mock_insights(self, analysis_results: Dict[str, Any], 
                              min_insights: int) -> List[Dict[str, Any]]:
        """Generate mock insights when LLM Council is unavailable"""
        logger.info("Generating mock insights (LLM Council not available)")
        
        insights = []
        
        # Generate insights from statistical tests
        statistical_tests = analysis_results.get("statistical_tests", [])
        significant_tests = [t for t in statistical_tests if t.get("significant", False)]
        
        for i, test in enumerate(significant_tests[:min(30, len(significant_tests))]):
            test_type = test.get("test", "unknown")
            interpretation = test.get("interpretation", "")
            
            insights.append({
                "id": f"mock_insight_{i + 1}",
                "title": f"Significant {test_type} Finding",
                "type": "statistical_test",
                "what": f"{interpretation[:200]}",
                "why": f"Statistical test {test_type} was significant (p < 0.05), indicating a genuine pattern rather than random chance",
                "how": "This finding can be used to validate assumptions and guide further analysis",
                "recommendation": "Consider this result in your interpretation and subsequent analysis steps"
            })
        
        # Generate insights from models
        models = analysis_results.get("models", {}).get("models", {})
        for model_name, model_info in models.items():
            if isinstance(model_info, dict) and "metrics" in model_info:
                metrics = model_info["metrics"]
                test_r2 = metrics.get("test_r2", 0)
                
                if test_r2 > 0.7:
                    insights.append({
                        "id": f"mock_insight_{len(insights) + 1}",
                        "title": f"Strong {model_info.get('model_type', model_name)} Performance",
                        "type": "model_performance",
                        "what": f"{model_name} achieved R² = {test_r2:.3f} on test data",
                        "why": f"High R² score indicates {model_name} explains {test_r2*100:.0f}% of the variance in the target variable",
                        "how": "Use this model for predictions. The high R² suggests good fit to the data.",
                        "recommendation": "Consider using this model for production predictions."
                    })
        
        return insights[:min_insights]
    
    def _summarize_statistical_results(self, results: Dict[str, Any]) -> str:
        """Summarize analysis results for prompt"""
        summary_parts = []
        
        # Statistical tests
        if "statistical_tests" in results:
            tests = results["statistical_tests"]
            significant_tests = [t for t in tests if t.get("significant", False)]
            summary_parts.append(f"- Statistical Tests: {len(tests)} tests performed, {len(significant_tests)} significant")
            
            # Add key findings
            for test in significant_tests[:5]:
                summary_parts.append(f"  - {test.get('test', 'Unknown')}: {test.get('interpretation', '')[:100]}")
        
        # Models
        if "models" in results:
            models = results["models"]
            if "models" in models:
                model_count = len(models["models"])
                summary_parts.append(f"- Predictive Models: {model_count} models built")
                
                # Add best model info
                if models.get("best_model"):
                    best = models["best_model"]
                    if "metrics" in best:
                        metrics = best["metrics"]
                        summary_parts.append(f"  - Best Model: {metrics.get('model_type', 'Unknown')}")
                        summary_parts.append(f"  - Performance: {metrics.get('interpretation', '')[:100]}")
        
        # Correlations
        if "statistical_tests" in results:
            correlations = [t for t in results["statistical_tests"] if t.get('test') == 'correlation']
            if correlations:
                strong_corrs = [c for c in correlations if abs(c.get('correlation_coefficient', 0)) >= 0.5]
                summary_parts.append(f"- Strong Correlations: {len(strong_corrs)} found")
        
        return "\n".join(summary_parts)
    
    def _guess_insight_type(self, text: str) -> str:
        """Guess insight type from text content"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['correlation', 'relationship', 'associated', 'linked']):
            return "correlation"
        elif any(word in text_lower for word in ['distribution', 'normal', 'skew']):
            return "distribution"
        elif any(word in text_lower for word in ['outlier', 'anomaly', 'unusual']):
            return "outlier"
        elif any(word in text_lower for word in ['model', 'predict', 'forecast', 'accuracy', 'performance']):
            return "model_performance"
        elif any(word in text_lower for word in ['missing', 'data quality', 'clean']):
            return "data_quality"
        elif any(word in text_lower for word in ['significant', 'test', 'p-value', 'statistic']):
            return "statistical_test"
        else:
            return "general"
    
    async def rank_models_with_council(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use LLM Council to rank and select best models
        
        Args:
            model_results: Dictionary with model names and their metrics
        
        Returns:
            Dictionary with ranking and recommendation
        """
        if not self.enabled:
            logger.info("LLM Council disabled, returning empty ranking")
            return {"council_used": False}
        
        logger.info("Ranking models with LLM Council consensus")
        
        # Build model summary for prompt
        model_summary = "\n".join([
            f"- {name}: {metrics.get('model_type', name)}"
            f"  Metrics: {metrics}"
            for name, metrics in model_results.items()
        ])
        
        prompt = f"""You are a team of machine learning experts evaluating predictive models.

Models Evaluated:
{model_summary}

Your task is to:
1. Evaluate each model's performance based on metrics
2. Rank models from best to worst
3. Provide a recommendation for which model to use

Consider:
- Accuracy/performance metrics (R², accuracy, F1-score, etc.)
- Model complexity and interpretability
- Training time and computational cost
- Suitability for use case (real-time, batch, interpretability vs accuracy tradeoff)

Provide your ranking as:
1. A numbered list from best (1) to worst
2. A final recommendation with justification

Format:
EVALUATION:
[Your detailed evaluation of each model]

FINAL RANKING:
1. [Model Name]
2. [Model Name]
...

RECOMMENDATION:
[Model name] with [justification]
"""
        
        # Check if we have API manager available
        if not self.api_manager:
            logger.warning("API manager not available, cannot use LLM Council")
            return {"council_used": False}
        
        # Use LLM Council for model ranking
        try:
            sys.path.insert(0, self.council_backend_path)
            # Import the local council module
            import council
            
            # Run LLM Council for model ranking
            stage1_results, stage2_results, stage3_result, metadata = await council.run_full_council(prompt)
            
            # Parse final ranking and recommendation
            result = {
                "council_used": True,
                "individual_evaluations": stage2_results,
                "final_synthesis": stage3_result,
                "ranking": [],
                "recommendation": {}
            }
            
            if stage3_result and 'response' in stage3_result:
                response_text = stage3_result['response']
                
                # Parse ranking
                ranking = council.parse_ranking_from_text(response_text)
                result['ranking'] = ranking
                
                # Parse recommendation
                import re
                if "RECOMMENDATION:" in response_text:
                    rec_section = response_text.split("RECOMMENDATION:")[-1]
                    
                    # Try to extract model name
                    for model_name in model_results.keys():
                        if model_name.lower() in rec_section.lower():
                            result['recommendation'] = {
                                "model": model_name,
                                "justification": rec_section[:300]
                            }
                            break
            
            logger.info(f"Model ranking completed: {len(result['ranking'])} models ranked")
            return result
            
        except Exception as e:
            logger.error(f"Error in LLM Council model ranking: {e}")
            return {"council_used": False, "error": str(e)}
    
    def enable(self):
        """Enable LLM Council"""
        self.enabled = True
        logger.info("LLM Council enabled")
    
    def disable(self):
        """Disable LLM Council"""
        self.enabled = False
        logger.info("LLM Council disabled")
    
    def is_enabled(self) -> bool:
        """Check if LLM Council is enabled"""
        return self.enabled


class EnhancedAnalysisPipeline:
    """Enhanced analysis pipeline with LLM Council integration"""
    
    def __init__(self, dataset_path: str, use_council: bool = True,
                 council_backend_path: str = None):
        """
        Initialize enhanced pipeline with LLM Council integration
        
        Args:
            dataset_path: Path to dataset
            use_council: Whether to use LLM Council for consensus
            council_backend_path: Path to llm-council backend
        """
        # Import pipeline components
        from workflow import AnalysisPipeline
        from analysis_engine import LLMCouncilAdapter
        
        # Base pipeline
        self.base_pipeline = AnalysisPipeline(dataset_path)
        
        # LLM Council integration
        self.use_council = use_council
        self.council_adapter = LLMCouncilAdapter(council_backend_path)
        
        logger.info(f"Enhanced pipeline initialized (Council: {use_council})")
    
    @property
    def dataset_name(self):
        return self.base_pipeline.dataset_name
    
    @property
    def output_dir(self):
        return self.base_pipeline.output_dir
    
    async def generate_hypotheses_async(self, max_hypotheses: int = 100) -> List[Dict[str, Any]]:
        """
        Generate hypotheses (with or without LLM Council)
        
        Args:
            max_hypotheses: Maximum hypotheses to generate
        
        Returns:
            List of hypotheses
        """
        if self.use_council:
            # Use LLM Council for consensus-based hypothesis generation
            dataset_info = self.base_pipeline.results.get('dataset_info', {})
            return await self.council_adapter.generate_hypotheses_with_council(
                dataset_info, max_hypotheses
            )
        else:
            # Use Agentic approach with tools
            from agents import create_hypothesis_generator_agent
            import json
            
            agent = create_hypothesis_generator_agent()
            dataset_info = self.base_pipeline.results.get('dataset_info', {})
            
            prompt = f"""Analyze this dataset and generate {max_hypotheses} testable hypotheses.
            Dataset Info: {dataset_info}
            
            Use your tools to explore the data if needed.
            Return the hypotheses as a JSON list of objects with fields: type, columns, hypothesis, test_method, reasoning.
            """
            
            try:
                result = agent.run(prompt)
                # Parse result if it's a string containing JSON
                if isinstance(result, str):
                    import re
                    json_match = re.search(r'\[.*\]', result, re.DOTALL)
                    if json_match:
                        hypotheses = json.loads(json_match.group())
                        return hypotheses
                elif isinstance(result, list):
                    return result
            except Exception as e:
                logger.warning(f"Agentic hypothesis generation failed: {e}. Falling back to procedural method.")
            
            # Fallback to single LLM (original procedural behavior)
            from analysis_engine import HypothesisGenerator
            
            generator = HypothesisGenerator(self.base_pipeline.df)
            return generator.generate_all_hypotheses(max_hypotheses)
    
    async def extract_insights_async(self, min_insights: int = 50) -> List[Dict[str, Any]]:
        """
        Extract insights (with or without LLM Council)
        
        Args:
            min_insights: Minimum insights to generate
        
        Returns:
            List of insights
        """
        if self.use_council:
            # Use LLM Council for consensus-based insight generation
            return await self.council_adapter.generate_insights_with_council(
                self.base_pipeline.results, min_insights
            )
        else:
            # Use Agentic approach with tools
            from agents import create_analyzer_agent
            import json
            
            agent = create_analyzer_agent()
            analysis_results = {
                'statistical_tests': self.base_pipeline.results['statistical_tests'],
                'modeling': self.base_pipeline.results.get('models', {})
            }
            
            prompt = f"""Review these analysis results and extract at least {min_insights} actionable insights.
            Results: {analysis_results}
            
            Use your tools to further explore the data if needed.
            Return the insights as a JSON list of objects with fields: title, type, what, why, how, recommendation.
            """
            
            try:
                result = agent.run(prompt)
                if isinstance(result, str):
                    import re
                    json_match = re.search(r'\[.*\]', result, re.DOTALL)
                    if json_match:
                        insights = json.loads(json_match.group())
                        return insights
                elif isinstance(result, list):
                    return result
            except Exception as e:
                logger.warning(f"Agentic insight extraction failed: {e}. Falling back to procedural method.")
            
            # Fallback to single LLM (original procedural behavior)
            from analysis_engine import InsightExtractor
            
            extractor = InsightExtractor(self.base_pipeline.df)
            
            analysis_results = {
                'correlations': [r for r in self.base_pipeline.results['statistical_tests'] 
                               if r.get('test') == 'correlation'],
                'distributions': [r for r in self.base_pipeline.results['statistical_tests'] 
                                 if r.get('test') == 'normality'],
                'outliers': [r for r in self.base_pipeline.results['statistical_tests'] 
                             if r.get('test') == 'outliers'],
                'statistical_tests': self.base_pipeline.results['statistical_tests'],
                'modeling': self.base_pipeline.results.get('models', {})
            }
            
            return extractor.generate_all_insights(analysis_results)
    
    async def rank_models_async(self) -> Dict[str, Any]:
        """
        Rank models using LLM Council consensus
        
        Returns:
            Dictionary with ranking and recommendation
        """
        if self.use_council and self.base_pipeline.results.get('models'):
            models = self.base_pipeline.results['models'].get('models', {})
            return await self.council_adapter.rank_models_with_council(models)
        else:
            return {"council_used": False}
    
    async def run_full_pipeline_with_council(self, target_column: str = None,
                                          generate_word: bool = True) -> Dict[str, Any]:
        """
        Run full pipeline with LLM Council integration
        
        Args:
            target_column: Target variable for modeling
            generate_word: Whether to generate Word document
        
        Returns:
            Dictionary with all results including council metadata
        """
        logger.info("Running full pipeline with LLM Council integration")
        
        # Run base pipeline steps
        self.base_pipeline.load_data()
        self.base_pipeline.clean_data()
        
        # Generate hypotheses with council
        hypotheses = await self.generate_hypotheses_async(max_hypotheses=100)
        self.base_pipeline.results['hypotheses'] = hypotheses
        self.base_pipeline.results['used_council_for_hypotheses'] = self.use_council
        
        # Initialize and update heuristic generator for report formatting
        from analysis_engine import HypothesisGenerator
        self.base_pipeline.heuristic_generator = HypothesisGenerator(self.base_pipeline.df)
        self.base_pipeline.heuristic_generator.hypotheses = hypotheses
        
        # Run statistical tests
        self.base_pipeline.run_statistical_tests()
        
        # Build models
        self.base_pipeline.build_models(target_column)
        
        # Rank models with council
        model_ranking = await self.rank_models_async()
        self.base_pipeline.results['model_ranking'] = model_ranking
        
        # Extract insights with council
        insights = await self.extract_insights_async(min_insights=50)
        self.base_pipeline.results['insights'] = insights
        self.base_pipeline.results['used_council_for_insights'] = self.use_council
        
        # Initialize and update insight extractor for report formatting
        from analysis_engine import InsightExtractor
        self.base_pipeline.insight_extractor = InsightExtractor(self.base_pipeline.df)
        self.base_pipeline.insight_extractor.insights = insights
        
        # Create visualizations
        self.base_pipeline.create_visualizations()
        
        # Generate reports
        self.base_pipeline.generate_reports(formats=['markdown', 'word'] if generate_word else ['markdown'])
        
        # Save execution log
        self.base_pipeline.save_execution_log()
        
        # Log token usage if available
        if HAS_API_MANAGER and self.council_adapter.api_manager:
            total_cost = self.council_adapter.api_manager.get_total_cost()
            logger.info(f"Total estimated API cost: ${total_cost:.6f}")
        
        return self.base_pipeline.results
