"""
LLM Council Integration for Data Science System
Integrates the LLM Council (multi-agent consensus) into analysis workflow
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
import logging

logger = logging.getLogger("LLMCouncilIntegration")


class LLMCouncilAdapter:
    """Adapter for LLM Council to work with Data Science System"""
    
    def __init__(self, council_backend_path: str = None):
        """
        Initialize LLM Council adapter
        
        Args:
            council_backend_path: Path to llm-council backend
        """
        self.council_backend_path = council_backend_path or "/home/engine/project/llm-council"
        self.enabled = True
        
        logger.info(f"LLM Council adapter initialized with backend: {self.council_backend_path}")
    
    async def generate_hypotheses_with_council(self, dataset_info: Dict[str, Any],
                                           max_hypotheses: int = 50) -> List[Dict[str, Any]]:
        """
        Generate hypotheses using LLM Council consensus
        
        Args:
            dataset_info: Dataset information (shape, columns, types, etc.)
            max_hypotheses: Maximum number of hypotheses to generate
        
        Returns:
            List of hypotheses generated by council consensus
        """
        if not self.enabled:
            logger.info("LLM Council disabled, using single LLM")
            return []
        
        logger.info("Generating hypotheses with LLM Council consensus")
        
        # Build prompt for hypothesis generation
        prompt = f"""You are a team of expert data scientists analyzing a dataset.

Dataset Information:
- Shape: {dataset_info.get('shape', 'Unknown')}
- Columns: {dataset_info.get('columns', [])}
- Data Types: {dataset_info.get('dtypes', {})}
- Missing Values: {dataset_info.get('missing_values', {})}

Your task is to generate {max_hypotheses} testable hypotheses about this dataset.

For each hypothesis, provide:
1. **Type**: correlation, distribution, categorical, outlier, or trend
2. **Columns involved**: Which columns this hypothesis relates to
3. **Hypothesis statement**: Clear testable statement
4. **Test method**: Statistical test to validate this hypothesis
5. **Reasoning**: Why this hypothesis is worth testing

Generate diverse hypotheses covering different aspects of the data. Focus on insights that would be actionable for a business or research context.

Format each hypothesis as a JSON object with these fields: type, columns, hypothesis, test_method, reasoning.
"""
        
        # Use LLM Council for consensus
        from llm_council.backend.council import run_full_council, parse_ranking_from_text
        
        stage1_results, stage2_results, stage3_result, metadata = await run_full_council(prompt)
        
        # Parse final synthesis for hypotheses
        hypotheses = []
        
        if stage3_result and 'response' in stage3_result:
            final_response = stage3_result['response']
            
            # Try to extract JSON from final response
            import json
            import re
            
            # Look for JSON blocks
            json_match = re.search(r'\{[^{}]*\}', final_response)
            
            if json_match:
                try:
                    # Parse as JSON array
                    hypotheses_data = json.loads(f"[{json_match.group()}]")
                    
                    if isinstance(hypotheses_data, list):
                        hypotheses = hypotheses_data[:max_hypotheses]
                    elif isinstance(hypotheses_data, dict):
                        # Single hypothesis
                        hypotheses_data['id'] = f"hypothesis_1"
                        hypotheses = [hypotheses_data]
                
                except json.JSONDecodeError:
                    logger.warning("Failed to parse hypotheses JSON from council response")
        
        # Fallback: use best individual response
        if not hypotheses and metadata.get('aggregate_rankings'):
            best_model = metadata['aggregate_rankings'][0] if metadata['aggregate_rankings'] else None
            
            if best_model:
                best_response = next(
                    (r['response'] for r in stage1_results if r['model'] == best_model['model']),
                    None
                )
                
                if best_response:
                    # Try to extract hypotheses from individual response
                    hypotheses = self._extract_hypotheses_from_text(best_response, max_hypotheses)
        
        logger.info(f"Generated {len(hypotheses)} hypotheses using LLM Council")
        return hypotheses
    
    def _extract_hypotheses_from_text(self, text: str, max_count: int) -> List[Dict[str, Any]]:
        """
        Extract hypotheses from unstructured text
        
        Args:
            text: Text containing hypotheses
            max_count: Maximum number to extract
        
        Returns:
            List of hypothesis dictionaries
        """
        import re
        
        hypotheses = []
        
        # Look for hypothesis patterns
        # Pattern: "Hypothesis [number]:" or numbered items
        numbered_items = re.findall(r'(?:Hypothesis|H)?\s*\d+\s*[:.]?\s*(.+?)(?=\n(?:\d+\.|Hypothesis|$))', text, re.IGNORECASE)
        
        for i, item in enumerate(numbered_items[:max_count], 1):
            # Try to identify type and content
            item_text = item.strip()
            
            hypothesis = {
                "id": f"consensus_hypothesis_{i}",
                "type": self._guess_hypothesis_type(item_text),
                "hypothesis": item_text,
                "test_method": "To be determined",
                "reasoning": "Generated by LLM Council consensus"
            }
            
            hypotheses.append(hypothesis)
        
        return hypotheses
    
    def _guess_hypothesis_type(self, text: str) -> str:
        """Guess hypothesis type from text content"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['correlation', 'relationship', 'associated', 'linked']):
            return "correlation"
        elif any(word in text_lower for word in ['distribution', 'normal', 'skewed', 'spread']):
            return "distribution"
        elif any(word in text_lower for word in ['category', 'group', 'segment', 'type']):
            return "categorical"
        elif any(word in text_lower for word in ['outlier', 'anomaly', 'extreme', 'unusual']):
            return "outlier"
        elif any(word in text_lower for word in ['trend', 'increase', 'decrease', 'over time', 'temporal']):
            return "trend"
        else:
            return "general"
    
    async def generate_insights_with_council(self, analysis_results: Dict[str, Any],
                                         min_insights: int = 50) -> List[Dict[str, Any]]:
        """
        Generate insights using LLM Council consensus
        
        Args:
            analysis_results: Analysis results (statistical tests, models, etc.)
            min_insights: Minimum number of insights to generate
        
        Returns:
            List of insights generated by council consensus
        """
        if not self.enabled:
            return []
        
        logger.info("Generating insights with LLM Council consensus")
        
        # Build prompt for insight generation
        statistical_summary = self._summarize_statistical_results(analysis_results)
        
        prompt = f"""You are a team of expert data analysts reviewing analysis results.

Analysis Summary:
{statistical_summary}

Your task is to generate at least {min_insights} actionable insights from these results.

For each insight, provide:
1. **Title**: Short descriptive title
2. **Type**: correlation, distribution, outlier, statistical_test, model_performance, or data_quality
3. **What**: Clear statement of the finding
4. **Why**: Explanation of the underlying reason (data-driven)
5. **How**: Practical application and how to use this insight
6. **Recommendation**: Actionable next steps

Focus on insights that are:
- Actionable for business or research decisions
- Backed by statistical evidence
- Explained in clear, non-technical language
- Have clear "why" and "how" components

Format each insight as a JSON object with these fields: title, type, what, why, how, recommendation.
"""
        
        # Use LLM Council
        from llm_council.backend.council import run_full_council
        
        stage1_results, stage2_results, stage3_result, metadata = await run_full_council(prompt)
        
        # Parse insights from final synthesis
        insights = []
        
        if stage3_result and 'response' in stage3_result:
            final_response = stage3_result['response']
            insights = self._extract_insights_from_text(final_response, min_insights)
        
        logger.info(f"Generated {len(insights)} insights using LLM Council")
        return insights
    
    def _summarize_statistical_results(self, results: Dict[str, Any]) -> str:
        """Summarize analysis results for prompt"""
        summary_parts = []
        
        # Statistical tests
        if 'statistical_tests' in results:
            tests = results['statistical_tests']
            significant_tests = [t for t in tests if t.get('significant', False)]
            summary_parts.append(f"- Statistical Tests: {len(tests)} tests performed, {len(significant_tests)} significant")
            
            # Add key findings
            for test in significant_tests[:5]:
                summary_parts.append(f"  - {test.get('test', 'Unknown')}: {test.get('interpretation', '')[:100]}")
        
        # Models
        if 'models' in results:
            models = results['models']
            if 'models' in models:
                model_count = len(models['models'])
                summary_parts.append(f"- Predictive Models: {model_count} models built")
                
                # Add best model info
                if models.get('best_model'):
                    best = models['best_model']
                    if 'metrics' in best:
                        metrics = best['metrics']
                        summary_parts.append(f"  - Best Model: {metrics.get('model_type', 'Unknown')}")
                        summary_parts.append(f"  - Performance: {metrics.get('interpretation', '')[:100]}")
        
        # Correlations
        if 'statistical_tests' in results:
            correlations = [t for t in results['statistical_tests'] if t.get('test') == 'correlation']
            if correlations:
                strong_corrs = [c for c in correlations if abs(c.get('correlation_coefficient', 0)) >= 0.5]
                summary_parts.append(f"- Strong Correlations: {len(strong_corrs)} found")
        
        return "\n".join(summary_parts)
    
    def _extract_insights_from_text(self, text: str, min_count: int) -> List[Dict[str, Any]]:
        """Extract insights from unstructured text"""
        import json
        import re
        
        insights = []
        
        # Try JSON extraction first
        json_matches = re.findall(r'\{[^{}]*\}', text)
        
        for match in json_matches:
            try:
                insight_data = json.loads(match)
                
                if isinstance(insight_data, dict):
                    insight_data['id'] = f"consensus_insight_{len(insights) + 1}"
                    insights.append(insight_data)
                elif isinstance(insight_data, list):
                    for item in insight_data:
                        item['id'] = f"consensus_insight_{len(insights) + 1}"
                        insights.append(item)
            except json.JSONDecodeError:
                continue
        
        # If we have enough insights from JSON, return them
        if len(insights) >= min_count:
            return insights[:min_count]
        
        # Fallback: extract numbered items
        numbered_items = re.findall(r'(?:Insight|I)?\s*\d+\s*[:.]?\s*(.+?)(?=\n(?:\d+\.|Insight|$))', text, re.IGNORECASE)
        
        for i, item in enumerate(numbered_items[:min_count - len(insights)], len(insights) + 1):
            item_text = item.strip()
            
            insight = {
                "id": f"consensus_insight_{i}",
                "type": self._guess_insight_type(item_text),
                "title": item_text[:100],
                "what": item_text,
                "why": "Generated by LLM Council consensus from analysis results",
                "how": "Apply findings according to context and requirements",
                "recommendation": "Use this insight to inform decision-making"
            }
            
            insights.append(insight)
        
        return insights[:min_count]
    
    def _guess_insight_type(self, text: str) -> str:
        """Guess insight type from text content"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['correlation', 'relationship', 'associated']):
            return "correlation"
        elif any(word in text_lower for word in ['distribution', 'normal', 'skew']):
            return "distribution"
        elif any(word in text_lower for word in ['outlier', 'anomaly', 'unusual']):
            return "outlier"
        elif any(word in text_lower for word in ['model', 'predict', 'forecast', 'accuracy']):
            return "model_performance"
        elif any(word in text_lower for word in ['missing', 'data quality', 'clean']):
            return "data_quality"
        elif any(word in text_lower for word in ['significant', 'test', 'p-value']):
            return "statistical_test"
        else:
            return "general"
    
    async def rank_models_with_council(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use LLM Council to rank and select best models
        
        Args:
            model_results: Dictionary with model names and their metrics
        
        Returns:
            Dictionary with ranking and recommendation
        """
        if not self.enabled:
            return {}
        
        logger.info("Ranking models with LLM Council consensus")
        
        # Build prompt for model ranking
        model_summary = "\n".join([
            f"- {name}: {metrics.get('model_type', name)}"
            f"  Metrics: {metrics}"
            for name, metrics in model_results.items()
        ])
        
        prompt = f"""You are a team of machine learning experts evaluating predictive models.

Models Evaluated:
{model_summary}

Your task is to:
1. Evaluate each model's performance based on the metrics
2. Rank models from best to worst
3. Provide a recommendation for which model to use

Consider:
- Accuracy/performance metrics
- Model complexity and interpretability
- Training time and computational cost
- Suitability for the use case

Provide your ranking as:
1. A numbered list from best (1) to worst
2. A final recommendation with justification

Format:
EVALUATION:
[Your evaluation of each model]

FINAL RANKING:
1. [Model Name]
2. [Model Name]
...

RECOMMENDATION:
[Model name] with [justification]
"""
        
        # Use LLM Council
        from llm_council.backend.council import run_full_council
        
        stage1_results, stage2_results, stage3_result, metadata = await run_full_council(prompt)
        
        # Parse final ranking and recommendation
        result = {
            "council_used": True,
            "individual_evaluations": stage2_results,
            "final_synthesis": stage3_result,
            "ranking": [],
            "recommendation": {}
        }
        
        if stage3_result and 'response' in stage3_result:
            response = stage3_result['response']
            result['ranking'] = self._parse_model_ranking(response)
            result['recommendation'] = self._parse_recommendation(response, list(model_results.keys()))
        
        return result
    
    def _parse_model_ranking(self, text: str) -> List[str]:
        """Parse model ranking from council response"""
        import re
        
        # Look for numbered list in "FINAL RANKING" section
        ranking_section = text
        
        if "FINAL RANKING:" in text:
            ranking_section = text.split("FINAL RANKING:")[-1]
        
        # Extract numbered items
        numbered_matches = re.findall(r'^\d+\.\s*(.+)$', ranking_section, re.MULTILINE)
        
        rankings = [match.strip() for match in numbered_matches if match.strip()]
        
        return rankings
    
    def _parse_recommendation(self, text: str, available_models: List[str]) -> Dict[str, str]:
        """Parse recommendation from council response"""
        import re
        
        result = {
            "model": "unknown",
            "justification": "No recommendation available"
        }
        
        if "RECOMMENDATION:" in text:
            rec_section = text.split("RECOMMENDATION:")[-1]
            
            # Try to match model name
            for model in available_models:
                if model.lower() in rec_section.lower():
                    result['model'] = model
                    result['justification'] = rec_section[:200]
                    break
        
        return result
    
    def enable(self):
        """Enable LLM Council"""
        self.enabled = True
        logger.info("LLM Council enabled")
    
    def disable(self):
        """Disable LLM Council"""
        self.enabled = False
        logger.info("LLM Council disabled")
    
    def is_enabled(self) -> bool:
        """Check if LLM Council is enabled"""
        return self.enabled


class EnhancedAnalysisPipeline:
    """Enhanced analysis pipeline with LLM Council integration"""
    
    def __init__(self, dataset_path: str, use_council: bool = True,
                 council_backend_path: str = None):
        """
        Initialize enhanced pipeline with LLM Council
        
        Args:
            dataset_path: Path to dataset
            use_council: Whether to use LLM Council for consensus
            council_backend_path: Path to llm-council backend
        """
        from workflow import AnalysisPipeline
        
        # Base pipeline
        self.base_pipeline = AnalysisPipeline(dataset_path)
        
        # LLM Council integration
        self.use_council = use_council
        self.council_adapter = LLMCouncilAdapter(council_backend_path)
        
        logger.info(f"Enhanced pipeline initialized (Council: {use_council})")
    
    async def generate_hypotheses_async(self, max_hypotheses: int = 100) -> List[Dict[str, Any]]:
        """
        Generate hypotheses (with or without LLM Council)
        
        Args:
            max_hypotheses: Maximum hypotheses to generate
        
        Returns:
            List of hypotheses
        """
        if self.use_council:
            # Use LLM Council for consensus-based hypothesis generation
            dataset_info = self.base_pipeline.results['dataset_info']
            return await self.council_adapter.generate_hypotheses_with_council(
                dataset_info, max_hypotheses
            )
        else:
            # Use single LLM (original behavior)
            from analysis_engine import HypothesisGenerator
            
            generator = HypothesisGenerator(self.base_pipeline.df)
            return generator.generate_all_hypotheses(max_hypotheses)
    
    async def extract_insights_async(self, min_insights: int = 50) -> List[Dict[str, Any]]:
        """
        Extract insights (with or without LLM Council)
        
        Args:
            min_insights: Minimum insights to generate
        
        Returns:
            List of insights
        """
        if self.use_council:
            # Use LLM Council for consensus-based insight generation
            return await self.council_adapter.generate_insights_with_council(
                self.base_pipeline.results, min_insights
            )
        else:
            # Use single LLM (original behavior)
            from analysis_engine import InsightExtractor
            
            extractor = InsightExtractor(self.base_pipeline.df)
            
            analysis_results = {
                'correlations': [r for r in self.base_pipeline.results['statistical_tests'] 
                               if r.get('test') == 'correlation'],
                'distributions': [r for r in self.base_pipeline.results['statistical_tests'] 
                                 if r.get('test') == 'normality'],
                'outliers': [r for r in self.base_pipeline.results['statistical_tests'] 
                             if r.get('test') == 'outliers'],
                'statistical_tests': self.base_pipeline.results['statistical_tests'],
                'modeling': self.base_pipeline.results.get('models', {})
            }
            
            return extractor.generate_all_insights(analysis_results)
    
    async def rank_models_async(self) -> Dict[str, Any]:
        """
        Rank models using LLM Council consensus
        
        Returns:
            Dictionary with ranking and recommendation
        """
        if self.use_council and self.base_pipeline.results.get('models'):
            models = self.base_pipeline.results['models'].get('models', {})
            return await self.council_adapter.rank_models_with_council(models)
        else:
            return {"council_used": False}
    
    async def run_full_pipeline_with_council(self, target_column: str = None,
                                          generate_word: bool = True) -> Dict[str, Any]:
        """
        Run full pipeline with LLM Council integration
        
        Args:
            target_column: Target variable for modeling
            generate_word: Whether to generate Word document
        
        Returns:
            Dictionary with all results including council metadata
        """
        logger.info("Running full pipeline with LLM Council integration")
        
        # Run base pipeline steps
        self.base_pipeline.load_data()
        self.base_pipeline.clean_data()
        
        # Generate hypotheses with council
        hypotheses = await self.generate_hypotheses_async(max_hypotheses=100)
        self.base_pipeline.results['hypotheses'] = hypotheses
        self.base_pipeline.results['used_council_for_hypotheses'] = self.use_council
        
        # Run statistical tests
        self.base_pipeline.run_statistical_tests()
        
        # Build models
        self.base_pipeline.build_models(target_column)
        
        # Rank models with council
        model_ranking = await self.rank_models_async()
        self.base_pipeline.results['model_ranking'] = model_ranking
        
        # Extract insights with council
        insights = await self.extract_insights_async(min_insights=50)
        self.base_pipeline.results['insights'] = insights
        self.base_pipeline.results['used_council_for_insights'] = self.use_council
        
        # Create visualizations
        self.base_pipeline.create_visualizations()
        
        # Generate reports
        self.base_pipeline.generate_reports(formats=['markdown', 'word'] if generate_word else ['markdown'])
        
        # Save execution log
        self.base_pipeline.save_execution_log()
        
        return self.base_pipeline.results
